{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical learning final project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset selection and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daniel A.\n",
    "### UID: 100444499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from autoimpute.imputations import MiceImputer\n",
    "import pycountry\n",
    "from difflib import get_close_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to importing the data I have made a find and replace with the following regex: \\\"\\s\\[[\\w\\S]{1,}\\]\\\", in order to remove some tags that the world bank databank adds to their columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the raw data\n",
    "raw_data = pd.read_csv('./data/raw/wb_raw_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding aggregates\n",
    "codes_to_exclude = raw_data.iloc[2593:2640,3].values\n",
    "\n",
    "# filtering the dataset\n",
    "data = raw_data[~(raw_data['Country Code'].isin(codes_to_exclude))] \n",
    "\n",
    "# removing final diagnostic columns\n",
    "data = data[~(data['Time'].isna()) & ~(data['Time Code'].isna())]\n",
    "\n",
    "# converting year column to integer\n",
    "data['Time'] = data['Time'].astype(int)\n",
    "\n",
    "# replacing .. with NAN as the raw data indends this to be a NAN\n",
    "data = data.replace('..',pd.NA)\n",
    "\n",
    "# sorting values\n",
    "data = data.sort_values(['Time','Country Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2002 2004 2005 2008 2010 2012 2013 2015 2018 2020]\n"
     ]
    }
   ],
   "source": [
    "# checking years we have queried\n",
    "years = data.Time.unique()\n",
    "print(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a dictionary with the subsets of the main dataframe\n",
    "dfs = {}\n",
    "for year in years:\n",
    "    dfs[year] = data[data['Time'] == year].reset_index(drop=True)\n",
    "\n",
    "# replacing nans in the 2020 dataframe with previous years data, as \n",
    "# the previous years' data still serves us a purpose for the analysis\n",
    "for year in years:   \n",
    "    dfs[2020] = dfs[2020].fillna(dfs[year])\n",
    "\n",
    "# removing columns where there's too many NANs\n",
    "cols_to_keep = []\n",
    "df = {}\n",
    "for col,val in zip(dfs[2020].columns,dfs[2020].isna().sum()):\n",
    "    if val < 45:\n",
    "        cols_to_keep.append(col)\n",
    "df = dfs[2020][cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking which countries have the most NANs\n",
    "all_countries = df['Country Name'].values\n",
    "countries_removed = []\n",
    "for country,val in zip(all_countries,df.isna().sum(axis=1)):\n",
    "    # removing at 3 nans per row\n",
    "    if val > 4:\n",
    "        countries_removed.append(country)\n",
    "\n",
    "# finally filtering to remove them\n",
    "# either way, these countries are mostly dependencies or \n",
    "# complex countries to get data from, like North Korea\n",
    "# so even after imputing, this would probably\n",
    "# yield unrealistic values\n",
    "df = df[~(df['Country Name'].isin(countries_removed))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping the wikipedia page for list of countries by human development index\n",
    "wikipage = requests.get('https://en.wikipedia.org/wiki/List_of_countries_by_Human_Development_Index').text\n",
    "soup = BeautifulSoup(wikipage,'lxml')\n",
    "\n",
    "# get the HDI table\n",
    "table = soup.find('table',{'class':'wikitable sortable'})\n",
    "links = table.findAll('a')\n",
    "tds = table.findAll('td')\n",
    "countries, hdi = [], []\n",
    "\n",
    "# going through the links to find the countries wikipedia has data for\n",
    "for link in links:\n",
    "    countries.append(link.get('title'))\n",
    "\n",
    "# cleaning countriesif not string\n",
    "countries = [x for x in countries if x != None]\n",
    "\n",
    "# finding the HDIs and appending them to the list hdi\n",
    "for td in tds:\n",
    "    try:\n",
    "        num = float(td.text)\n",
    "        if str(num)[0:2] == '0.':\n",
    "            hdi.append(num)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the HDI to the dataframe for the countries which we have in the wikipedia list\n",
    "df['HDI'] = pd.NA\n",
    "for n,country in enumerate(countries):\n",
    "    df.loc[df['Country Name'] == country,'HDI'] = hdi[n]\n",
    "\n",
    "# not all country names are written the same, we must find the country names\n",
    "# using ISO 3166 country code and then finding those that match\n",
    "missing_codes = list(df[~df['Country Name'].isin(countries)]['Country Code'].values)\n",
    "for code in missing_codes:\n",
    "    country = pycountry.countries.get(alpha_3=code)\n",
    "    if country != None and country.name in countries: \n",
    "        idx = countries.index(country.name)\n",
    "        df.loc[df['Country Code'] == code, 'HDI'] = hdi[idx]\n",
    "\n",
    "        # remove the code from the list when done\n",
    "        missing_codes.remove(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[['Cuba'],\n ['The Bahamas', 'Panama'],\n [],\n [],\n ['Cape Verde'],\n ['Solomon Islands', 'Marshall Islands'],\n ['Democratic Republic of the Congo'],\n ['Togo', 'Mongolia', 'Tonga'],\n [],\n [],\n ['Zambia', 'Namibia', 'The Gambia'],\n ['Serbia'],\n [],\n ['Iceland', 'Iran', 'Poland'],\n ['Czech Republic', 'Dominican Republic'],\n [],\n ['Malta'],\n ['Federated States of Micronesia'],\n [],\n [],\n [],\n ['São Tomé and Príncipe'],\n [],\n ['Saint Kitts and Nevis', 'Saint Vincent and the Grenadines'],\n ['Saint Vincent and the Grenadines', 'Saint Kitts and Nevis'],\n ['Dominican Republic'],\n [],\n [],\n []]"
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to deal with the rest of the missing values\n",
    "# we take the most important word of the country and find it in countries\n",
    "missing_names = []\n",
    "for code in missing_codes:\n",
    "    country = pycountry.countries.get(alpha_3=code)\n",
    "    if country != None:\n",
    "        missing_names.append(get_close_matches(country.name,countries))\n",
    "missing_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}