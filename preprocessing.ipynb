{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical learning final project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset selection and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daniel A.\n",
    "### UID: 100444499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from autoimpute.imputations import MiceImputer\n",
    "import pycountry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to importing the data I have made a find and replace with the following regex: \\\"\\s\\[[\\w\\S]{1,}\\]\\\", in order to remove some tags that the world bank databank adds to their columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the raw data\n",
    "raw_data = pd.read_csv('./data/raw/wb_raw_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding aggregates\n",
    "codes_to_exclude = raw_data.iloc[2593:2640,3].values\n",
    "\n",
    "# filtering the dataset\n",
    "data = raw_data[~(raw_data['Country Code'].isin(codes_to_exclude))] \n",
    "\n",
    "# removing final diagnostic columns\n",
    "data = data[~(data['Time'].isna()) & ~(data['Time Code'].isna())]\n",
    "\n",
    "# converting year column to integer\n",
    "data['Time'] = data['Time'].astype(int)\n",
    "\n",
    "# replacing .. with NAN as the raw data indends this to be a NAN\n",
    "data = data.replace('..',pd.NA)\n",
    "\n",
    "# sorting values\n",
    "data = data.sort_values(['Time','Country Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2002 2004 2005 2008 2010 2012 2013 2015 2018 2020]\n"
     ]
    }
   ],
   "source": [
    "# checking years we have queried\n",
    "years = data.Time.unique()\n",
    "print(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a dictionary with the subsets of the main dataframe\n",
    "dfs = {}\n",
    "for year in years:\n",
    "    dfs[year] = data[data['Time'] == year].reset_index(drop=True)\n",
    "\n",
    "# replacing nans in the 2020 dataframe with previous years data, as \n",
    "# the previous years' data still serves us a purpose for the analysis\n",
    "for year in years:   \n",
    "    dfs[2020] = dfs[2020].fillna(dfs[year])\n",
    "\n",
    "# removing columns where there's too many NANs\n",
    "cols_to_keep = []\n",
    "df = {}\n",
    "for col,val in zip(dfs[2020].columns,dfs[2020].isna().sum()):\n",
    "    if val < 45:\n",
    "        cols_to_keep.append(col)\n",
    "df = dfs[2020][cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking which countries have the most NANs\n",
    "all_countries = df['Country Name'].values\n",
    "countries_removed = []\n",
    "for country,val in zip(all_countries,df.isna().sum(axis=1)):\n",
    "    # removing at 3 nans per row\n",
    "    if val > 4:\n",
    "        countries_removed.append(country)\n",
    "\n",
    "# finally filtering to remove them\n",
    "# either way, these countries are mostly dependencies or \n",
    "# complex countries to get data from, like North Korea\n",
    "# so even after imputing, this would probably\n",
    "# yield unrealistic values\n",
    "df = df[~(df['Country Name'].isin(countries_removed))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping the wikipedia page for list of countries by human development index\n",
    "hdis_page = requests.get('https://en.wikipedia.org/wiki/List_of_countries_by_Human_Development_Index').text\n",
    "soup = BeautifulSoup(hdis_page,'lxml')\n",
    "\n",
    "\n",
    "# get the HDI table\n",
    "table = soup.find('table',{'class':'wikitable sortable'})\n",
    "links = table.findAll('a')\n",
    "tds = table.findAll('td')\n",
    "countries, hdis = [], []\n",
    "\n",
    "# going through the links to find the countries wikipedia has data for\n",
    "for link in links:\n",
    "    countries.append(link.get('title'))\n",
    "\n",
    "# cleaning countriesif not string\n",
    "countries = [x for x in countries if x != None]\n",
    "\n",
    "# finding the HDIs and appending them to the list hdi\n",
    "for td in tds:\n",
    "    try:\n",
    "        num = float(td.text)\n",
    "        if str(num)[0:2] == '0.':\n",
    "            hdis.append(num)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# making a dict with the values\n",
    "countries = {cont:['',hdi] for cont,hdi in zip(countries,hdis)}\n",
    "\n",
    "# using the same approach for the codes for easier matching later on\n",
    "codes_page = requests.get('https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3').text\n",
    "soup = BeautifulSoup(codes_page,'lxml')\n",
    "\n",
    "# get the codes table\n",
    "table = soup.find('div',{'class':'plainlist'})\n",
    "# scrape list elements and add them to the dict\n",
    "lis = table.findAll('li')\n",
    "li_lists = []\n",
    "for li in lis:\n",
    "    try:\n",
    "        countries[li.find('a').get('title')][0] = li.find('span').text\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# making the values tuples\n",
    "countries = {cont:tuple(val) for cont,val in countries.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the HDI to the dataframe for the countries which we have in the wikipedia list\n",
    "df['HDI'] = pd.NA\n",
    "matched_countries = []\n",
    "missing_codes = []\n",
    "for code,hdi in countries.values():\n",
    "    df.loc[df['Country Code'] == code,'HDI'] = hdi\n",
    "\n",
    "# finally keeping this dataframe\n",
    "df = df[df['Country Code'].isin([x[0] for x in countries.values()])].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing the rest of the missing values\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}